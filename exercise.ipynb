{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cacb372",
   "metadata": {},
   "source": [
    "# Exercise 05: Image Segmentation\n",
    "\n",
    "<hr style=\"height:2px;\">\n",
    "\n",
    "In this notebook, we will train a 2D U-Net for nuclei segmentation in the Kaggle Nuclei dataset.\n",
    "\n",
    "Written by Valentyna Zinchenko, Constantin Pape and William Patton."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b8a665",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "Please use kernel 05-image-segmentation for this exercise.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a14836",
   "metadata": {},
   "source": [
    "Our goal is to produce a model that can take an image as input and produce a segmentation as shown in this table.\n",
    "\n",
    "| Image | Mask |\n",
    "| :-: | :-: |\n",
    "| ![image](static/image.png) | ![mask](static/mask.png) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20a9a7e",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;\">\n",
    "\n",
    "## The libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93388e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext tensorboard\n",
    "import os\n",
    "from pathlib import Path\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6b3dfc",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;\">\n",
    "\n",
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429d340b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Data exploration\n",
    "For this exercise we will be using the Kaggle 2018 Data Science Bowl data.\n",
    "We will try to segment it with a state of the art network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c081ce",
   "metadata": {},
   "source": [
    "Make sure that the data was successfully extracted:\n",
    "if everything went fine, you should have folders `nuclei_train_data` and `nuclei_val_data`\n",
    "in your working directory. Let's check if it is the case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7fbb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all of the directories\n",
    "list([x for x in Path().iterdir() if x.is_dir()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f875ec19",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <p><b>Task 1.1: </b>: Explore the contents of both folders. Running `ls your_folder_name`\n",
    "    should display you what is stored in the folder of your interest.</p>\n",
    "    </p>You should be familiar with how the images are stored and the storage format.</p>\n",
    "    <b>Questions:</b>\n",
    "    <ol>\n",
    "        <li>How many image/mask pairs are there in the training/validation set?</li>\n",
    "        <li>What is the file type of the images/masks?</li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2b84f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answers here:\n",
    "num_train_pairs: int = ...\n",
    "num_val_pairs: int = ...\n",
    "image_file_type: str = \".***\"\n",
    "mask_file_type: str = \".***\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376f6980",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <p><b>Task 1.2: </b>: Visualize the image associated with the following mask:</p>\n",
    "    <p>Hint: you can use the following function to display an image:</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4d7c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_one_image(image_path):\n",
    "    image = imageio.imread(image_path)\n",
    "    plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e7e564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show mask\n",
    "show_one_image(\n",
    "    Path(\n",
    "        \"nuclei_train_data/f29fd9c52e04403cd2c7d43b6fe2479292e53b2f61969d25256d2d2aca7c6a81/mask.tif\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28187dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show image\n",
    "show_one_image(Path(...))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19054e46",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Data Processing\n",
    "Making the data accessible for training. What one would normally start with in any machine learning pipeline is writing a dataset - a class that will fetch the training samples. Once you switch to using your own data, you would have to figure out how to fetch the data yourself. Luckily most of the functionality is already provided by PyTorch, but what you need to do is to write a class, that will actually supply the dataloader with training samples - a Dataset.\n",
    "\n",
    "Torch Dataset docs can be found [here](https://pytorch.org/docs/stable/data.html?highlight=dataset#torch.utils.data.Dataset) and a totorial on how to use them can be found [here](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#dataset-class).\n",
    "\n",
    "The main idea: any Dataset class should implement two \"magic\" methods:\n",
    "1) `__len__(self)`: this defines the behavior of the python builtin `len` function. i.e:\n",
    "    `len(dataset)` => number of elements in your dataset\n",
    "2) `__getitem__(self, idx)`: this defines bracket indexing behavior of your class. i.e:\n",
    "    `dataset[idx]` => element of your dataset associated with `idx`\n",
    "\n",
    "For this exercise you will not have to do it yourself yet, but please carefully read through the provided class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c09dfb5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# any PyTorch dataset class should inherit the initial torch.utils.data.Dataset\n",
    "class NucleiDataset(Dataset):\n",
    "    \"\"\"A PyTorch dataset to load cell images and nuclei masks\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, transform=None, img_transform=None):\n",
    "        self.root_dir = root_dir  # the directory with all the training samples\n",
    "        self.samples = os.listdir(root_dir)  # list the samples\n",
    "        self.transform = (\n",
    "            transform  # transformations to apply to both inputs and targets\n",
    "        )\n",
    "        self.img_transform = img_transform  # transformations to apply to raw image only\n",
    "        #  transformations to apply just to inputs\n",
    "        self.inp_transforms = transforms.Compose(\n",
    "            [\n",
    "                transforms.Grayscale(),  # some of the images are RGB\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.5], [0.5]), # 0.5 = mean and 0.5 = variance \n",
    "            ]\n",
    "        )\n",
    "\n",
    "    # get the total number of samples\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    # fetch the training sample given its index\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root_dir, self.samples[idx], \"image.tif\")\n",
    "        # we'll be using Pillow library for reading files\n",
    "        # since many torchvision transforms operate on PIL images\n",
    "        image = Image.open(img_path)\n",
    "        image = self.inp_transforms(image)\n",
    "        mask_path = os.path.join(self.root_dir, self.samples[idx], \"mask.tif\")\n",
    "        mask = transforms.ToTensor()(Image.open(mask_path))\n",
    "        if self.transform is not None:\n",
    "            # Note: using seeds to ensure the same random transform is applied to\n",
    "            # the image and mask\n",
    "            seed = torch.seed()\n",
    "            torch.manual_seed(seed)\n",
    "            image = self.transform(image)\n",
    "            torch.manual_seed(seed)\n",
    "            mask = self.transform(mask)\n",
    "        if self.img_transform is not None:\n",
    "            image = self.img_transform(image)\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6932f29e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <p><b>Task 1.3</b>: Use the defined dataset to show a random image/mask pair</p>\n",
    "    <p>Hint: use the <code>len</code> function and <code>[]</code> indexing defined by <code>__len__</code> and\n",
    "    <code>__get_index__</code> in the Dataset class to fill in the function below.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5388355",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_random_dataset_image(dataset):\n",
    "    idx = ...\n",
    "    img, mask = ...\n",
    "    f, axarr = plt.subplots(1, 2)  # make two plots on one figure\n",
    "    axarr[0].imshow(img[0])  # show the image\n",
    "    axarr[1].imshow(mask[0], interpolation=None)  # show the masks\n",
    "    _ = [ax.axis(\"off\") for ax in axarr]  # remove the axes\n",
    "    print(\"Image size is %s\" % {img[0].shape})\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e75872",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA_PATH = \"nuclei_train_data\"\n",
    "train_data = NucleiDataset(TRAIN_DATA_PATH)\n",
    "\n",
    "show_random_dataset_image(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47f029d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "As you can probably see, if you clicked enough times, some of the images are really huge! What happens if we load them into memory and run the model on them? We might run out of memory. That's why normally, when training networks on images or volumes one has to be really careful about the sizes. In practice, you would want to regulate their size. Additional reason for restraining the size is: if we want to train in batches (faster and more stable training), we need all the images in the batch to be of the same size. That is why we prefer to either resize or crop them.\n",
    "\n",
    "Here is a function (well, actually a class), that will apply a transformation 'random crop'. Notice that we apply it to images and masks simultaneously to make sure they correspond, despite the randomness.\n",
    "\n",
    "Why do we bother making a bulky class to handle the relatively simple task of loading images? We want to keep the code modular. We want to write one dataset object, and then we can try all the possible transforms with this one dataset. Similarly, we want to write one Randomcrop transform object, and then we can reuse it for any other image datasets we night have in the future.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907aa152",
   "metadata": {},
   "source": [
    "PS: PyTorch already has quite a bunch of all possible data transforms, so if you need one, check [here](https://pytorch.org/vision/stable/transforms.html#transforms-on-pil-image-and-torch-tensor). The biggest problem with them is that they are clearly separated into transforms applied to PIL images (remember, we initially load the images as PIL.Image?) and torch.tensors (remember, we converted the images into tensors by calling transforms.ToTensor()?). This can be incredibly annoying if for some reason you might need to transorm your images to tensors before applying any other transforms or you don't want to use PIL library at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c327baeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = NucleiDataset(TRAIN_DATA_PATH, transforms.RandomCrop(256))\n",
    "train_loader = DataLoader(train_data, batch_size=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04ea462",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_random_dataset_image(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ea3714",
   "metadata": {},
   "source": [
    "And the same for the validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea831e93",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "VAL_DATA_PATH = \"nuclei_val_data\"\n",
    "val_data = NucleiDataset(VAL_DATA_PATH, transforms.RandomCrop(256))\n",
    "val_loader = DataLoader(val_data, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b1b479",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_random_dataset_image(val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad466e4f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<h2> Checkpoint 1 </h2>\n",
    "<p>We will go over the steps up to this point soon. If you have time to spare, consider experimenting\n",
    "with various augmentations. Your goal is to augment your training data in such a way that you expand\n",
    "the distribution of training data to cover the distribution of the rest of your data and avoid overly\n",
    "relying on extrapolation at test time. If this sounds somewhat vague thats because augmenting is a\n",
    "bit of an artform. Common augmentations that have been shown to work well are adding noise, mirror,\n",
    "transpose, and rotations.</p>\n",
    "<p>Note that some of these transformations need to be applied to both the raw image and the segmentation,\n",
    "wheras others should only be applied to the image (i.e. noise augmentation). The Dataset <code>__init__</code>\n",
    "function takes 2 arguments, <code>transform</code> and <code>img_transform</code> so that you can define\n",
    "a set of transformations that you only want to apply to the image.</p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1907d3cc",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "augmented_data = NucleiDataset(\n",
    "    TRAIN_DATA_PATH,\n",
    "    transforms.RandomCrop(256),\n",
    "    img_transform=transforms.Compose([transforms.GaussianBlur(5)]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc04f694",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_random_dataset_image(augmented_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cb1834",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba2de6b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## The model: U-Net\n",
    "\n",
    "Now we need to define the architecture of the model to use. We will use a [U-Net](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/) that has proven to steadily outperform the other architectures in segmenting biological and medical images.\n",
    "\n",
    "The image of the model precisely describes all the building blocks you need to use to create it. All of them can be found in the list of PyTorch layers (modules) [here](https://pytorch.org/docs/stable/nn.html#convolution-layers).\n",
    "\n",
    "The U-Net has an encoder-decoder structure:\n",
    "\n",
    "In the encoder pass, the input image is successively downsampled via max-pooling. In the decoder pass it is upsampled again via transposed convolutions.\n",
    "\n",
    "In adddition, it has skip connections, that bridge the output from an encoder to the corresponding decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842016ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    \"\"\"UNet implementation\n",
    "    Arguments:\n",
    "      in_channels: number of input channels\n",
    "      out_channels: number of output channels\n",
    "      final_activation: activation applied to the network output\n",
    "    \"\"\"\n",
    "\n",
    "    # _conv_block and _upsampler are just helper functions to\n",
    "    # construct the model.\n",
    "    # encapsulating them like so also makes it easy to re-use\n",
    "    # the model implementation with different architecture elements\n",
    "\n",
    "    # Convolutional block for single layer of the decoder / encoder\n",
    "    # we apply two 2d convolutions with relu activation\n",
    "    def _conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    # upsampling via transposed 2d convolutions\n",
    "    def _upsampler(self, in_channels, out_channels):\n",
    "        return nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
    "\n",
    "    def __init__(self, in_channels=1, out_channels=1, depth=4, final_activation=None):\n",
    "        super().__init__()\n",
    "\n",
    "        assert depth < 10, \"Max supported depth is 9\"\n",
    "\n",
    "        # the depth (= number of encoder / decoder levels) is\n",
    "        # hard-coded to 4\n",
    "        self.depth = depth\n",
    "\n",
    "        # the final activation must either be None or a Module\n",
    "        if final_activation is not None:\n",
    "            assert isinstance(\n",
    "                final_activation, nn.Module\n",
    "            ), \"Activation must be torch module\"\n",
    "\n",
    "        # all lists of conv layers (or other nn.Modules with parameters) must be wraped\n",
    "        # itnto a nn.ModuleList\n",
    "\n",
    "        # modules of the encoder path\n",
    "        self.encoder = nn.ModuleList(\n",
    "            [\n",
    "                self._conv_block(in_channels, 16),\n",
    "                self._conv_block(16, 32),\n",
    "                self._conv_block(32, 64),\n",
    "                self._conv_block(64, 128),\n",
    "                self._conv_block(128, 256),\n",
    "                self._conv_block(256, 512),\n",
    "                self._conv_block(512, 1024),\n",
    "                self._conv_block(1024, 2048),\n",
    "                self._conv_block(2048, 4096),\n",
    "            ][:depth]\n",
    "        )\n",
    "        # the base convolution block\n",
    "        if depth >= 1:\n",
    "            self.base = self._conv_block(2 ** (depth + 3), 2 ** (depth + 4))\n",
    "        else:\n",
    "            self.base = self._conv_block(1, 2 ** (depth + 4))\n",
    "        # modules of the decoder path\n",
    "        self.decoder = nn.ModuleList(\n",
    "            [\n",
    "                self._conv_block(8192, 4096),\n",
    "                self._conv_block(4096, 2048),\n",
    "                self._conv_block(2048, 1024),\n",
    "                self._conv_block(1024, 512),\n",
    "                self._conv_block(512, 256),\n",
    "                self._conv_block(256, 128),\n",
    "                self._conv_block(128, 64),\n",
    "                self._conv_block(64, 32),\n",
    "                self._conv_block(32, 16),\n",
    "            ][-depth:]\n",
    "        )\n",
    "\n",
    "        # the pooling layers; we use 2x2 MaxPooling\n",
    "        self.poolers = nn.ModuleList([nn.MaxPool2d(2) for _ in range(self.depth)])\n",
    "        # the upsampling layers\n",
    "        self.upsamplers = nn.ModuleList(\n",
    "            [\n",
    "                self._upsampler(8192, 4096),\n",
    "                self._upsampler(4096, 2048),\n",
    "                self._upsampler(2048, 1024),\n",
    "                self._upsampler(1024, 512),\n",
    "                self._upsampler(512, 256),\n",
    "                self._upsampler(256, 128),\n",
    "                self._upsampler(128, 64),\n",
    "                self._upsampler(64, 32),\n",
    "                self._upsampler(32, 16),\n",
    "            ][-depth:]\n",
    "        )\n",
    "        # output conv and activation\n",
    "        # the output conv is not followed by a non-linearity, because we apply\n",
    "        # activation afterwards\n",
    "        self.out_conv = nn.Conv2d(16, out_channels, 1)\n",
    "        self.activation = final_activation\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = input\n",
    "        # apply encoder path\n",
    "        encoder_out = []\n",
    "        for level in range(self.depth):\n",
    "            x = self.encoder[level](x)\n",
    "            encoder_out.append(x)\n",
    "            x = self.poolers[level](x)\n",
    "\n",
    "        # apply base\n",
    "        x = self.base(x)\n",
    "\n",
    "        # apply decoder path\n",
    "        encoder_out = encoder_out[::-1]\n",
    "        for level in range(self.depth):\n",
    "            x = self.upsamplers[level](x)\n",
    "            x = self.decoder[level](torch.cat((x, encoder_out[level]), dim=1))\n",
    "\n",
    "        # apply output conv and activation (if given)\n",
    "        x = self.out_conv(x)\n",
    "        if self.activation is not None:\n",
    "            x = self.activation(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8d5237",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Task 2.1</b>: Spot the best U-Net\n",
    "\n",
    "In the next cell you fill find a series of UNet definitions. Most of them won't work. Some of them will work but not well. One will do well. Can you identify which model is the winner? Unfortunately you can't yet test your hypotheses yet since we have not covered loss functions, optimizers, and train/validation loops.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454ab129",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "unetA = UNet(\n",
    "    in_channels=1, out_channels=1, depth=4, final_activation=torch.nn.Sigmoid()\n",
    ")\n",
    "unetB = UNet(in_channels=1, out_channels=1, depth=9, final_activation=None)\n",
    "unetC = torch.nn.Sequential(\n",
    "    UNet(in_channels=1, out_channels=1, depth=4, final_activation=torch.nn.ReLU()),\n",
    "    torch.nn.Sigmoid(),\n",
    ")\n",
    "unetD = torch.nn.Sequential(\n",
    "    UNet(in_channels=1, out_channels=1, depth=1, final_activation=None),\n",
    "    torch.nn.Sigmoid(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e01249e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide your guesses as to what, if anything, might go wrong with each of these models:\n",
    "#\n",
    "# unetA: The correct unet.\n",
    "#\n",
    "# unetB: Too deep! You won't be able to train with input size 256 since the lowest level will get zero sized tensors.\n",
    "#\n",
    "# unetC: A classic mistake putting a Sigmoid after a Relu activation. You will never predict anything < 0.5\n",
    "#\n",
    "# unetD: Barely any depth to this U-Net. It should train and give you what you want, I just wouldn't expect good performance\n",
    "\n",
    "favorite_unet: UNet = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e753c828",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371346f0",
   "metadata": {},
   "source": [
    "## Loss Function\n",
    "\n",
    "The next step to do would be writing a loss function - a metric that will tell us how close we are to the desired output. This metric should be differentiable, since this is the value to be backpropagated. The are [multiple losses](https://lars76.github.io/2018/09/27/loss-functions-for-segmentation.html) we could use for the segmentation task.\n",
    "\n",
    "Take a moment to think which one is better to use. If you are not sure, don't forget that you can always google! Before you start implementing the loss yourself, take a look at the [losses](https://pytorch.org/docs/stable/nn.html#loss-functions) already implemented in PyTorch. You can also look for implementations on GitHub."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ef86ea",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Task 2.2</b>: Implement your loss (or take one from pytorch):\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15736c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement your loss here or initialize the one of your choice from pytorch\n",
    "loss_function: torch.nn.Module = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07d25a4",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    Test your loss function here, is it behaving as you'd expect?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943ba7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = torch.tensor([0.0, 1.0])\n",
    "good_prediction = torch.tensor([0.01, 0.99])\n",
    "bad_prediction = torch.tensor([0.4, 0.6])\n",
    "wrong_prediction = torch.tensor([0.9, 0.1])\n",
    "\n",
    "good_loss = loss_function(good_prediction, target)\n",
    "bad_loss = loss_function(bad_prediction, target)\n",
    "wrong_loss = loss_function(wrong_prediction, target)\n",
    "\n",
    "assert good_loss < bad_loss\n",
    "assert bad_loss < wrong_loss\n",
    "\n",
    "# Can your loss function handle predictions outside of (0, 1)?\n",
    "# Some loss functions will be perfectly happy with this which may\n",
    "# make them easier to work with, but predictions outside the expected\n",
    "# range will not work well with our soon to be discussed evaluation metric.\n",
    "out_of_bounds_prediction = torch.tensor([-0.1, 1.1])\n",
    "\n",
    "try:\n",
    "    oob_loss = loss_function(out_of_bounds_prediction, target)\n",
    "    print(\"Your loss supports out-of-bounds predictions.\")\n",
    "except RuntimeError as e:\n",
    "    print(e)\n",
    "    print(\"Your loss does not support out-of-bounds predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c1bddb",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "We will use the [Dice Coefficient](https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient) to evaluate the network predictions.\n",
    "We can use it for validation if we interpret set $a$ as predictions and $b$ as labels. It is often used to evaluate segmentations with sparse foreground, because the denominator normalizes by the number of foreground pixels.\n",
    "The Dice Coefficient is closely related to Jaccard Index / Intersection over Union."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ac1425",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Task 2.3</b>: Fill in implementation details for the Dice Coefficient\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876210f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorensen Dice Coefficient implemented in torch\n",
    "# the coefficient takes values in two discrete arrays\n",
    "# with values in {0, 1}, and produces a score in [0, 1]\n",
    "# where 0 is the worst score, 1 is the best score\n",
    "class DiceCoefficient(nn.Module):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "\n",
    "    # the dice coefficient of two sets represented as vectors a, b ca be\n",
    "    # computed as (2 *|a b| / (a^2 + b^2))\n",
    "    def forward(self, prediction, target):\n",
    "        intersection = ...\n",
    "        union = ...\n",
    "        return 2 * intersection / union.clamp(min=self.eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416f142b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    Test your Dice Loss here, are you getting the right scores?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d838efc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dice = DiceCoefficient()\n",
    "target = torch.tensor([0.0, 1.0])\n",
    "good_prediction = torch.tensor([0.0, 1.0])\n",
    "bad_prediction = torch.tensor([0.0, 0.0])\n",
    "wrong_prediction = torch.tensor([1.0, 0.0])\n",
    "\n",
    "assert dice(good_prediction, target) == 1.0, dice(good_prediction, target)\n",
    "assert dice(bad_prediction, target) == 0.0, dice(bad_prediction, target)\n",
    "assert dice(wrong_prediction, target) == 0.0, dice(wrong_prediction, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d965a98",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Task 2.4</b>: What happes if your predictions are not discrete elements of {0,1}?\n",
    "    <ol>\n",
    "        <li>What if the predictions are in range (0,1)?</li>\n",
    "        <li>What if the predictions are in range ($-\\infty$,$\\infty$)?</li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59eaef0c",
   "metadata": {},
   "source": [
    "Answer:\n",
    "1) ...\n",
    "\n",
    "2) ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3db37d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <h2>Checkpoint 2</h2>\n",
    "\n",
    "This is a good place to stop for a moment. If you have extra time look into some extra loss functions or try to implement your own if you haven't yet. You could also explore alternatives for evaluation metrics since there are alternatives that you could look into.\n",
    "\n",
    "</div>\n",
    "\n",
    "<hr style=\"height:2px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40435146",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Let's start with writing training and validation functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1cbe5a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Task 3.1</b>: Fix in all the TODOs to make the train function work. If confused, you can use this [PyTorch tutorial](https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html) as a template\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe658fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply training for one epoch\n",
    "def train(\n",
    "    model,\n",
    "    loader,\n",
    "    optimizer,\n",
    "    loss_function,\n",
    "    epoch,\n",
    "    log_interval=100,\n",
    "    log_image_interval=20,\n",
    "    tb_logger=None,\n",
    "    device=None,\n",
    "    early_stop=False,\n",
    "):\n",
    "    if device is None:\n",
    "        # You can pass in a device or we will default to using\n",
    "        # the gpu. Feel free to try training on the cpu to see\n",
    "        # what sort of performance difference there is\n",
    "        if torch.cuda.is_available():\n",
    "            device = torch.device(\"cuda\")\n",
    "        else:\n",
    "            device = torch.device(\"cpu\")\n",
    "\n",
    "    # set the model to train mode\n",
    "    model.train()\n",
    "\n",
    "    # move model to device\n",
    "    model = model.to(device)\n",
    "\n",
    "    # iterate over the batches of this epoch\n",
    "    for batch_id, (x, y) in enumerate(loader):\n",
    "        # move input and target to the active device (either cpu or gpu)\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        # apply model and calculate loss\n",
    "        prediction = ...  # placeholder since we use prediction later\n",
    "        loss = ...  # placeholder since we use the loss later\n",
    "\n",
    "        # log to console\n",
    "        if batch_id % log_interval == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_id * len(x),\n",
    "                    len(loader.dataset),\n",
    "                    100.0 * batch_id / len(loader),\n",
    "                    loss.item(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # log to tensorboard\n",
    "        if tb_logger is not None:\n",
    "            step = epoch * len(loader) + batch_id\n",
    "            tb_logger.add_scalar(\n",
    "                tag=\"train_loss\", scalar_value=loss.item(), global_step=step\n",
    "            )\n",
    "            # check if we log images in this iteration\n",
    "            if step % log_image_interval == 0:\n",
    "                tb_logger.add_images(\n",
    "                    tag=\"input\", img_tensor=x.to(\"cpu\"), global_step=step\n",
    "                )\n",
    "                tb_logger.add_images(\n",
    "                    tag=\"target\", img_tensor=y.to(\"cpu\"), global_step=step\n",
    "                )\n",
    "                tb_logger.add_images(\n",
    "                    tag=\"prediction\",\n",
    "                    img_tensor=prediction.to(\"cpu\").detach(),\n",
    "                    global_step=step,\n",
    "                )\n",
    "\n",
    "        if early_stop and batch_id > 5:\n",
    "            print(\"Stopping test early!\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed248eaa",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    Quick sanity check for your train function to make sure no errors are thrown:\n",
    "    Good place to test unetA, unetB, unetC, unetD to see if you can eliminate some\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d9fa25",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "simple_net = UNet(1, 1, depth=1, final_activation=nn.Sigmoid())\n",
    "\n",
    "train(\n",
    "    simple_net,\n",
    "    train_loader,\n",
    "    optimizer=torch.optim.Adam(simple_net.parameters()),\n",
    "    loss_function=torch.nn.MSELoss(),\n",
    "    epoch=0,\n",
    "    log_interval=1,\n",
    "    early_stop=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63a6237",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Task 3.2</b>: Fix in all the TODOs to make the validate function work. If confused, you can use this <a href=\"https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html\">PyTorch tutorial</a> as a template\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f62c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run validation after training epoch\n",
    "def validate(\n",
    "    model,\n",
    "    loader,\n",
    "    loss_function,\n",
    "    metric,\n",
    "    step=None,\n",
    "    tb_logger=None,\n",
    "    device=None,\n",
    "):\n",
    "    if device is None:\n",
    "        # You can pass in a device or we will default to using\n",
    "        # the gpu. Feel free to try training on the cpu to see\n",
    "        # what sort of performance difference there is\n",
    "        if torch.cuda.is_available():\n",
    "            device = torch.device(\"cuda\")\n",
    "        else:\n",
    "            device = torch.device(\"cpu\")\n",
    "\n",
    "    # set model to eval mode\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    # running loss and metric values\n",
    "    val_loss = 0\n",
    "    val_metric = 0\n",
    "\n",
    "    # disable gradients during validation\n",
    "    with torch.no_grad():\n",
    "        # iterate over validation loader and update loss and metric values\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            # TODO: evaluate this example with the given loss and metric\n",
    "            prediction = ...\n",
    "            val_loss += ...\n",
    "            val_metric += ...\n",
    "\n",
    "    # normalize loss and metric\n",
    "    val_loss /= len(loader)\n",
    "    val_metric /= len(loader)\n",
    "\n",
    "    if tb_logger is not None:\n",
    "        assert (\n",
    "            step is not None\n",
    "        ), \"Need to know the current step to log validation results\"\n",
    "        tb_logger.add_scalar(tag=\"val_loss\", scalar_value=val_loss, global_step=step)\n",
    "        tb_logger.add_scalar(\n",
    "            tag=\"val_metric\", scalar_value=val_metric, global_step=step\n",
    "        )\n",
    "        # we always log the last validation images\n",
    "        tb_logger.add_images(tag=\"val_input\", img_tensor=x.to(\"cpu\"), global_step=step)\n",
    "        tb_logger.add_images(tag=\"val_target\", img_tensor=y.to(\"cpu\"), global_step=step)\n",
    "        tb_logger.add_images(\n",
    "            tag=\"val_prediction\", img_tensor=prediction.to(\"cpu\"), global_step=step\n",
    "        )\n",
    "\n",
    "    print(\n",
    "        \"\\nValidate: Average loss: {:.4f}, Average Metric: {:.4f}\\n\".format(\n",
    "            val_loss, val_metric\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91c87db",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    Quick sanity check for your train function to make sure no errors are thrown\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cece36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_net = UNet(1, 1, depth=1, final_activation=None)\n",
    "\n",
    "# build the dice coefficient metric\n",
    "\n",
    "validate(\n",
    "    simple_net,\n",
    "    train_loader,\n",
    "    loss_function=torch.nn.MSELoss(),\n",
    "    metric=DiceCoefficient(),\n",
    "    step=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82a3365",
   "metadata": {},
   "source": [
    "\n",
    "We want to use GPU to train faster. Make sure GPU is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb793f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7e803b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start a tensorboard writer\n",
    "logger = SummaryWriter('runs/Unet')\n",
    "%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569fd1c6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Use the unet you expect to work the best!\n",
    "model = favorite_unet\n",
    "\n",
    "# use adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# build the dice coefficient metric\n",
    "metric = DiceCoefficient()\n",
    "\n",
    "# train for $25$ epochs\n",
    "# during the training you can inspect the\n",
    "# predictions in the tensorboard\n",
    "n_epochs = 25\n",
    "for epoch in range(n_epochs):\n",
    "    # train\n",
    "    train(\n",
    "        model,\n",
    "        train_loader,\n",
    "        optimizer=optimizer,\n",
    "        loss_function=loss_function,\n",
    "        epoch=epoch,\n",
    "        log_interval=5,\n",
    "        tb_logger=logger,\n",
    "    )\n",
    "    step = epoch * num_train_pairs\n",
    "    # validate\n",
    "    validate(model, val_loader, loss_function, metric, step=step, tb_logger=logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccd8fdc",
   "metadata": {},
   "source": [
    "Your validation metric was probably around 85% by the end of the training. That sounds good enough, but an equally important thing to check is:\n",
    "Open the Images tab in your Tensorboard and compare predictions to targets. Do your predictions look reasonable? Are there any obvious failure cases?\n",
    "If nothing is clearly wrong, let's see if we can still improve the model performance by changing the model or the loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f61fc4d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <h2>Checkpoint 3</h2>\n",
    "\n",
    "This is the end of the guided exercise. We will go over all of the code up until this point shortly. While you wait you are encouraged to try alternative loss functions, evaluation metrics, augmentations, and networks.\n",
    "After this come additional exercises if you are interested and have the time.\n",
    "\n",
    "</div>\n",
    "<hr style=\"height:2px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb55d88",
   "metadata": {},
   "source": [
    "## Additional Exercises\n",
    "\n",
    "1. Modify and evaluate the following architecture variants of the U-Net:\n",
    "    * use [GroupNorm](https://pytorch.org/docs/stable/nn.html#torch.nn.GroupNorm) to normalize convolutional group inputs\n",
    "    * use more layers in your UNet.\n",
    "\n",
    "2. Use the Dice Coefficient as loss function. Before we only used it for validation, but it is differentiable and can thus also be used as loss. Compare to the results from exercise 2.\n",
    "Hint: The optimizer we use finds minima of the loss, but the minimal value for the Dice coefficient corresponds to a bad segmentation. How do we need to change the Dice Coefficient to use it as loss nonetheless?\n",
    "\n",
    "3. Compare the results of these trainings to the first one. If any of the modifications you've implemented show better results, combine them (e.g. add both GroupNorm and one more layer) and run trainings again.\n",
    "What is the best result you could get?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab42f94e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Task BONUS 4.1</b>: Group Norm, update the U-Net to use a GroupNorm layer\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81b270f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetGN(UNet):\n",
    "    \"\"\"\n",
    "    A subclass of UNet that implements GroupNorm in each convolutional block\n",
    "    \"\"\"\n",
    "\n",
    "    # Convolutional block for single layer of the decoder / encoder\n",
    "    # we apply two 2d convolutions with relu activation\n",
    "    def _conv_block(self, in_channels, out_channels):\n",
    "        # See the original UNet for an example of how to build the convolutional block\n",
    "        # We want operation -> activation -> normalization (2x)\n",
    "        # Hint: Group norm takes a \"num_groups\" argument. Use 8 to match the solution\n",
    "        return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c83a7f5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "model = UNetGN(1, 1, final_activation=nn.Sigmoid())\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "metric = DiceCoefficient()\n",
    "\n",
    "logger = SummaryWriter(\"runs/UNetGN\")\n",
    "\n",
    "\n",
    "# train for 40 epochs\n",
    "# during the training you can inspect the\n",
    "# predictions in the tensorboard\n",
    "n_epochs = 40\n",
    "for epoch in range(n_epochs):\n",
    "    train(\n",
    "        model,\n",
    "        train_loader,\n",
    "        optimizer=optimizer,\n",
    "        loss_function=loss_function,\n",
    "        epoch=epoch,\n",
    "        log_interval=5,\n",
    "        tb_logger=logger,\n",
    "    )\n",
    "    step = epoch * num_train_pairs\n",
    "    validate(model, val_loader, loss_function, metric, step=step, tb_logger=logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5330dae",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Task BONUS 4.2</b>: More Layers\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87e825a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with more layers. For example UNet with depth 5\n",
    "\n",
    "model = ...\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "metric = DiceCoefficient()\n",
    "\n",
    "loss = torch.nn.BCELoss()\n",
    "\n",
    "logger = SummaryWriter(\"runs/UNet5layers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717508f0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# train for 25 epochs\n",
    "# during the training you can inspect the\n",
    "# predictions in the tensorboard\n",
    "n_epochs = 25\n",
    "for epoch in range(n_epochs):\n",
    "    train(\n",
    "        model,\n",
    "        train_loader,\n",
    "        optimizer=optimizer,\n",
    "        loss_function=loss,\n",
    "        epoch=epoch,\n",
    "        log_interval=5,\n",
    "        tb_logger=logger,\n",
    "    )\n",
    "    step = epoch * num_train_pairs\n",
    "    validate(model, val_loader, loss, metric, step=step, tb_logger=logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23530f2a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Task BONUS 4.3</b>: Dice Loss\n",
    "    Dice Loss is a simple inversion of the Dice Coefficient.\n",
    "    We already have a Dice Coefficient implementation, so now we just\n",
    "    need a layer that can invert it.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d270414",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    \"\"\" \"\"\"\n",
    "\n",
    "    def __init__(self, offset: float = 1):\n",
    "        super().__init__()\n",
    "        self.dice_coefficient = DiceCoefficient()\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03b9627",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Now combine the Dice Coefficient layer with the Invert layer to make a Dice Loss\n",
    "dice_loss = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b462f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with Dice Loss\n",
    "net = ...\n",
    "optimizer = ...\n",
    "metric = ...\n",
    "loss_func = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a40ebbb",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "logger = SummaryWriter(\"runs/UNet_diceloss\")\n",
    "\n",
    "n_epochs = 40\n",
    "for epoch in range(n_epochs):\n",
    "    train(\n",
    "        net,\n",
    "        train_loader,\n",
    "        optimizer=optimizer,\n",
    "        loss_function=loss_func,\n",
    "        epoch=epoch,\n",
    "        log_interval=5,\n",
    "        tb_logger=logger,\n",
    "    )\n",
    "    step = epoch * num_train_pairs\n",
    "    validate(net, val_loader, loss_func, metric, step=step, tb_logger=logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ff95dd",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Task BONUS 4.4</b>: Group Norm + Dice\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89988dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ...\n",
    "optimizer = ...\n",
    "metric = ...\n",
    "loss_func = ...\n",
    "\n",
    "logger = SummaryWriter(\"runs/UNetGN_diceloss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8729b1d1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "logger = SummaryWriter(\"runs/UNetGN_diceloss\")\n",
    "\n",
    "n_epochs = 40\n",
    "for epoch in range(n_epochs):\n",
    "    train(\n",
    "        net,\n",
    "        train_loader,\n",
    "        optimizer=optimizer,\n",
    "        loss_function=loss_func,\n",
    "        epoch=epoch,\n",
    "        log_interval=5,\n",
    "        tb_logger=logger,\n",
    "    )\n",
    "    step = epoch * num_train_pairs\n",
    "    validate(net, val_loader, loss_func, metric, step=step, tb_logger=logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107e8627",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Task BONUS 4.5</b>: Group Norm + Dice + U-Net 5 Layers\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e879a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ...\n",
    "optimizer = ...\n",
    "metric = ...\n",
    "loss_func = ..."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "all",
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
